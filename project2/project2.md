# Q-Learning Taxi-v3 Project

This project implements a **Q-learning agent** to solve the classic **Taxi-v3** environment from the Gymnasium library. The goal of the environment is for the taxi to pick up a passenger and deliver them to the correct destination while avoiding illegal moves and maximizing rewards.

---

## ğŸš€ Overview

The project uses **Q-learning**, an off-policy, model-free reinforcement learning algorithm. We train a Q-table that stores the expected future rewards for each **state-action** pair. Over many episodes, the agent learns an optimal policy for navigating the Taxi environment.

---

## ğŸ“¦ Requirements

Make sure you have the following installed:

```bash
gymnasium
numpy
```

Install Gymnasium (with classic control environments):

```bash
pip install gymnasium[classic-control]
```

---

## ğŸ§  Q-Learning Algorithm

Q-learning is based on the update rule:

```
Q(s,a) â† (1 âˆ’ Î±) * Q(s,a) + Î± * ( r + Î³ * max_a' Q(s',a') )
```

Where:

* **Î± (alpha)** â†’ Learning rate
* **Î³ (gamma)** â†’ Discount factor
* **r** â†’ Reward
* **s, a** â†’ Current state and action
* **s'** â†’ Next state
* **max Q(s', a')** â†’ Best future value

We use an **Îµ-greedy policy** to balance exploration and exploitation.

---

## ğŸ“„ Code Summary

Key features of the project:

* Initialize Q-table with zeros
* Use Îµ-greedy strategy to explore
* Update Q-values according to Q-learning equation
* Decay Îµ gradually each episode
* Render trained agent for visualization

---

## ğŸ“Š Hyperparameters

The main training hyperparameters used are:

```python
alpha = 0.9       # Learning rate
gamma = 0.95      # Discount factor
epsilon = 1.0     # Exploration rate
epsilon_decay = 0.995
min_epsilon = 0.01
num_episodes = 10000
max_steps = 100
```

These control how fast the agent learns, how much it explores, and when it begins exploiting its learned policy.

---

## ğŸ§ª Testing the Agent

After training, the agent is tested using `render_mode="human"` for visual feedback. The policy becomes fully greedy (uses only `argmax`).

---

## ğŸ“ Project Structure

```
ğŸ“¦ taxi-q-learning
 â”£ ğŸ“œ q_learning_taxi.py
 â”£ ğŸ“œ README.md
 â”— ğŸ“‚ results (optional)
```

---

## ğŸ“ˆ Possible Improvements

Here are some enhancements you can try:

* Plot reward per episode
* Implement SARSA for comparison
* Use Double Q-learning to reduce overestimation
* Convert the agent to a DQN using neural networks
* Add logging and evaluation metrics

---

## ğŸ§‘â€ğŸ’» Author

Amr Belal â€” Reinforcement Learning Student & Developer

Feel free to expand and build upon this project!
